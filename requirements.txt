# NanoTuner-3B-Q-Finetuning/requirements.txt

# Core ML Libraries
torch==2.3.0
transformers==4.42.0
accelerate==0.31.0 # Crucial for efficient model loading and multi-GPU (if applicable)
datasets==2.20.0
peft==0.11.1       # Parameter-Efficient Fine-Tuning (for LoRA/QLoRA)
trl==0.9.0         # Transformer Reinforcement Learning (includes SFTTrainer)
bitsandbytes==0.43.1 # Essential for 4-bit quantization (QLoRA)

# Optional for specialized kernels / speedup
# flash-attn is often pre-installed or not needed for smaller QLoRA runs
# flash-attn==2.5.8 

# Optional for logging/tracking (Recommended for research!)
wandb==0.17.0      # Weights & Biases (for hyperparameter tracking)
